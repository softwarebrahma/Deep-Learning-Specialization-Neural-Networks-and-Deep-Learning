probably say that the computations of a neural network are all organized in terms of a forward path or a forward propagation step in which we compute the output of the new network followed by a backward pass or a back complication step which we use to compute gradients or compute derivatives the computation graph explains why it is organized this way in this video we'll go through an example in order to illustrate the computation drought let's use a simpler example than logistic regression or a full-blown neural network let's say that we're trying to compute a function J which is a function of three variables a B and C and let's say that function is three times a plus B times C computing this function actually has three distinct steps the first is you need to compute what is B times C and let's say we store that in a variable called u so U is equal to B times C and then you might compute to be musical a times u so let's say you know this is V and then finally your output J is V times V so this is your final function J you trying to compute we can take these three steps and draw them in a computation graph as follows let's say I draw your three variables a B and C here so the first thing we did was compute u equals B times C I'm going to put a rectangular box around that and so the inputs of that are B and C and then you might have V equals a plus u so the inputs to that ah B so the inputs to that are you which we just computed together with a and then finally we have J equals three times B so as I can for example if equals five B equals V and C equals two then du equals BC would be six V equals a plus u be five plus six and eleven J is three times at so J is equal to 33 and and indeed hope you can verify that you know this is a three times five plus three times two and if you expand that out you know you actually get so these three has the value of J so the computation graph comes in handy when there is some distinguished or some special output variable such as J in this case that you want to optimize and in the case of the logistic regression J is of course the cost function that we're trying to minimize and what we've seen in this little example is that through a left-to-right pause you can compute the value of J and what we'll see in the next couple slides is that in order to compute derivatives Opa right to left pause like this kind of going in the opposite direction as the blue arrows that would be most natural for computing the derivatives so the recap the computation graph organizes a computation with this blue arrow left to right computation lets defer to the next video how you can do the backward red arrow right to left computation of the derivatives let's go on to the next video