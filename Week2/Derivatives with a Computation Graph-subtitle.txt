In the last video, we worked through an example of using a computation graph to compute the function J. Now, let's take a cleaned up version of that computation graph and show how you can use it to figure out derivative calculations for that function J. So, here's a computation graph. Let's say you want to compute the derivative of J with respect to V. So, what is that? Well, this says if we were to take this value of V and change it a little bit, how would the value of J change? Well, J is defined as three times V, and right now V is equal to 11. So, if we're to pump up V by a little bit to 11.001, then J, which is currently 33, would end up being pumped up to V times the new value of V, becomes 33.001. And so, if you pump a V by .001, then J which has three Vs and currently 33 will get pumped up to 33.003. So, here we've increased V by .001 and the net result of that is that J goes up three times as much. So the derivative of J with respect to V is equal to three because the increase in J is three times the increase in V. And, in fact, this is very analogous to the example we had in the previous video where we had f(a) equals 3a. And so, we then derive that df(a)/da which was slightly simplified and slightly sloppy notation, you can read as df/da was equal to three. So, instead, here we have J equals 3V and so dJ/dV is equal to three, with here J playing the role of F, and V playing the role of A in this previous example that we had right from an earlier video. In the terminology of backpropagation what we've seen is that if you want to compute the derivative of this final upper variable which uses variable you care most about, with respect to V, then we're done sort of one step of backpropagation so the called one step backwards in this graph. Now, let's look at another example. What is dJ/da? In other words, if we pump up the value of A, how does that affect the value of J? Well, let's go through the example. Variable A is equal to five. So let's pump it up to 5.001. The net impact of that is that V which was A plus U, so that was previous 11, this we can increase to 11.001. And then we've already seen as above that J now gets bumped up to 33.003. So, what we've seen is that if you increase A by 0.001, J increases by 0.003. And by increase A I mean if you were to take this value 5 and just plug in the new value then the change to A will propagate to the right of the computation graph. So that J ends up being 33.003. And so, the increase to J is three times the increase to A. That means this derivative is equal to three. One way to break this down is to say that if you change A then that would change V, and through changing V, that would change J. And so, the net change to the value of J, when you bump up the value, when you nudge the value of A up a little bit is that, first, by changing A you end up increasing V. Well, how much does V increase? It is increased by an amount that's determined by dV/dA and then the change in V will cause the value of J to also increase. So, in Calculus this is actually called the chain rule, that's if A affects J, then the amount that J changes when you nudge A is the product of how much V changes when you nudge A, times how much J changes when you nudge V. So in Calculus again this is called the chain rule. What we saw from this calculation is that if you increase A by 0.001, V changes by the same amount. So dV/dA is equal to one. So in fact if you plug in what we have worked up previously on dV/dJ is equal to three and dV/da is equal to one, so the product of this, three times one. That actually gives you the correct value that dJ/da is equal to three. This little illustration shows how by having computed dJ/dV had this derivative with respect to this variable, it can then help you to compute dJ/da. And so, that's another step of this backward calculation. I just want to introduce one more new notational convention, which is that when you're writing codes to implement backpropagation, there usually be some final output variable that you really care about, a final output variable that you really care about or that you want to optimize. And in this case, this final output variable is j. It's really the last note in your computation graph. And so, a lot of computations will be trying to compute the derivative of that find the output variable. So d of this final output variable with respect to some other variable. Let me just call that, d var. So, a lot of the computations you have would be to compute the derivative of the final output variable, letter j in this case, with various intermediate variable such as a, b, c, u, r, v. And when you implement this in software, what do you call this variable name? One thing you could do is, in Python, you could write a very long variable name, d Final Output Var over a d var. But that's a very long variable name. We could call this d_j, d var. But because you're always taking derivatives respect to d_j, respect to this final output variable, I'm going to introduce a new notation, where in code, when you're computing this thing in the code you write, we're just going to use the variable name d, v, a, r in order to represent that quantity. Okay? So d, v, a, r in the code you write, will represent the derivative of the final output variable you care about such as j, sometimes the last l with respect to the various intermediate quantities you're computing in your code. So this thing here in your code, you use d_v to denote this value. So d_v would be equal to three and your code represents this as a d_a, which is we also figured out to be equal to three. Okay? So we've done backpropagation partially through this computation graph, let's go through the rest of this example on the next slide. So let's go to clean up a copy of the computation graph. And just to recap, what we've done so far, is go backward here and figured out that d_v is equal to three. And again, the definition of d_v, that's just a variable name of the code is really d, j, d, v. I figured out that d_a is equal to three and again, d_a is the variable name in your code and that's really the value of d_j, d_a. Have a sort of hand wave how you have gone backwards on these two edges, like so. Now, let's keep computing derivatives. Let's look at the value, u. So what is d_j, d_u? Well, through a similar calculation as what we did before, now we start off with u equals six. If you bump up u to 6.001, then v which is previous 11, goes up to 11.001, and so j goes from 33 to 33.003. And so the increase in j is 3x, so this is equal. And the analysis for u is very similar to the analysis we did for a. This is actually computed as d_j, d_v times d_v, d_u. With this, we had already figured out was three, and this turns out to be equal to one. So we've got one more step of back propagation, we end up computing that d_u is also equal to three, and d_u is of course, just as d_j, d_u. Now, we just step through one last example in detail. So what is d_j, d_v? Imagine if you are allowed to change the value of b and you want to tweak b a little bit in order to minimize or maximize the value of j. So what is the derivative, what's the slope of this function j when you change the value of b a little bit? It turns out that, using the chain rule for calculus, this can be written as the product of two things, is d_j, d_u times d_u, d_v. And the reasoning is, if you change b a little bit, so b goes to 3 to, say, 3.001. The way it'll affect j is, it will first affect u. So how much does it affect u? Well, u is defined as b times c, right? So this will go from six when b is equal to three, to now, or 6.002. Right? Because c is equal to two, in our example here. And so this tells us that, d_u, d_b is equal to two because when you pump up b by .001, u increase twice as much. So d_u, d_b, this is equal to two. And now, we know that u has gone up twice as much as b has gone up. Well, what is d_j, d_u? We've already figured out that this is equal to three and so by multiplying these two hosts, we find that d_j,d_b is equal to six. And again, here's the reasoning for the second part of the argument, which is, we want to know when u goes up by .002, how does that affect j? The fact that d_j, d_u is equal to three, that tells us that when u goes up by .002, j goes up three times as much. So j should go up by .006, right? That comes from a fact that d_j, d_u is equal to three. And if you check the math in detail, you will find that, if b becomes 3.001, then u becomes 6.002, v becomes 11.002, so that's a plus u, that's five plus u. And then j, which is equal to three times v, that answer being equal to 33.006. Right? And so that's how you get that d_j, d_b is equal to six. And to fill that in, this is if we go backwards, so this is d_b is equal to six and d_b really is the Python code variable name for the d_j, d_b. And I won't go through the last example in great detail but it turns out that, if you also compute how d_j, d_a, this turns out to be d_j, d_u times d_u, d_a and this turns out to be nine. Just turns out to be three times three. I won't go through that example in detail. Through this last step, it is possible to derive that d_c is equal to 9. So the key takeaway from this video, from this example is that, when computing derivatives in computing all of these derivatives, the most efficient way to do so, is through a right to left computation following the direction of the red arrows. And in particular, we'll first compute the derivatives respect to v and then that becomes useful for computing the derivative respect a and the derivative respect to u. And then, derivative respect to u, for example, this term over here and this term over here, those, in turn, become useful for computing the derivative respect to b and the derivative respect to c. So that was a computation graph and how there's a forward or left to right calculation to compute the cost functions such as j, do you might want to optimize. And a backwards or a right to left calculation to compute derivatives. If you're not familiar with calculus or the chain rule, I know some of those details are gone by really quickly. But if you didn't follow all the details, don't worry about it. In the next video, we'll go over this again in the context of logistic regression, and show you exactly what you need to do in order to implement the computations you need to compute derivatives through the logistic regression model.