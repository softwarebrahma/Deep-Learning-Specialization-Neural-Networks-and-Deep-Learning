
Vectorizing Logistic Regression:

Using numPy's Vector x Matrix dot product function we can compute the forward propagation step of Logistic Regression's gradient descent "to compute predictions" over m training examples in a vectorized fashion as below,

As before stacking the feature vectors x in the training examples (x1,y1), (x2,y2),..,(xm,ym) in a horizontal fashion we get the matrix X which will be nx x m dimensional matrix (nx, m)

Then using the parameter vector w as a column vector we get the nx dimensional column vector (nx, 1)

Now we can perform Vector x Matrix dot product to get the Z row vector (1, m),

Z = np.dot(w.T, X) + b

In the above though b is a scalar, numPy internally converts it into an m dimensional row vector (1, m) using a method called "Broadcasting" to make the addition transparent.

Similarly we can compute the A row vector (1,m) as below,

A = np.sig(Z)


Vectorizing Logistic Regression's Gradient Output:

Using numPy's we can also compute the backward propagation step of Logistic Regression's gradient descent "to compute derivatives" over m training examples in a vectorized fashion as below, 

NOTE : We still need a for loop over the number of iterations for gradient descent (when we need multiple iterations of gradient descent).

Since we know from before that,

z = w'x + b
a = sigmoid(z)
dz = a - y
dw1 = x1 * dz
dw2 = x2 * dz
db = dz

and that,

J(w,b) = 1 / m * sum(L(a,y))
dw1 w.r.t J(w,b) = 1 / m * sum(dw1)
dw2 w.r.t J(w,b) = 1 / m * sum(dw2)
db w.r.t J(w,b) = 1 / m * sum(db)


We can now do the math with Vectorization as below,

DZ = A - Y
	- A is the m dimensional row vector (1, m) obtained by doing sigmoid of the m dimensional row vector, Z (from above).
	- Y is a m dimensional row vector (1, m) created by horizontal stacking of labels y from the training set.
	- DZ will then be a m dimensional vector (1, m)

DW = np.dot(X,DZ.T) / m
	- DW will then be a nx dimensional vector (nx, 1).

DB = np.sum(DZ) / m
	- DB will be a scalar.


With the above we can compute one step of gradient descent by,

W = W - alpha * DW

B = B - alpha * DB


Python:

In [1]: import numpy as np
... 

In [2]: x = np.array([2,4])

In [9]: print(x)
[2 4]

In [5]: print((x / 2))
[ 1.  2.]

In [6]: print((1 / 2 * x))
[ 1.  2.]



In [12]: y = np.array([[1,2],[3,4]])

In [21]: print(y)
[[1 2]
 [3 4]]

In [13]: print(np.shape(y))
(2, 2)

In [16]: y * x.T
Out[16]: 
array([[ 2,  8],
       [ 6, 16]])

In [17]: np.dot(y, x.T)
Out[17]: array([10, 22])

In [18]: print(x.T)
[2 4]

In [19]: print(np.shape(x.T))
(2,)

In [31]: print(x)
[2 4]

In [20]: print(np.shape(x))
(2,)


In [25]: z = np.array([[1,2],[3,4],[5,6]])

In [29]: print(z)
[[1 2]
 [3 4]
 [5 6]]

In [28]: print(np.shape(z))
(3, 2)

In [30]: print(z.T)
[[1 3 5]
 [2 4 6]]

In [27]: print(np.shape(z.T))
(2, 3)



