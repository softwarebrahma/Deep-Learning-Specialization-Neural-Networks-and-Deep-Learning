welcome back in this week's you learn to implement a neural network before diving into the technical details I wanted in this video to give you a quick overview of what you'll be seeing in this week's videos so if you don't follow all the details in this video don't worry about it we'll delve in the technical details in the next few videos but for now let's give a quick overview of how you implement in your network last week we had talked about logistic regression and we saw how this model corresponds to the following computation graph where you didn't put the features X and parameters W MB that allows you to compute Z which is then used to compute a and we were using a interchangeably with this output Y hat and then you can compute the loss function l a new network looks like this and as I'd already previously alluded you can form a neural network by stacking together a lot of little sigmoid units whereas previously this node corresponds to two steps of calculations the first three compute the Z value second is it computes this a value in this dual network this stack of notes will correspond to a Z like calculation like this as well as an a like calculation like that and then that node will correspond to another Z and another 8 like calculation so the notation which we should use later will look like this first what inputs the features X together with some parameters W and B and this will allow you to compute z1 so new notation that one should use is that we'll use a superscript square bracket 1 to refer to quantities associated with this stack of nodes called a lair and then later we'll use superscript square bracket 2 to refer to quantities associated with Daniel really that's called another layer of the network and the superscript square brackets like we have here are not to be confused with the superscript round brackets which we used to refer to individual training examples so whereas X supersu round bracket I referred to the I've trained example superscript square bracket 1 and 2 refers to these different um layers layer 1 and layer 2 in this network but they're going on after computing z1 similar to logistic regression there will be a computation to compute a 1 and that's just some sigmoid of z1 and then you compute Z 2 using another linear equation and then compute a 2 and a 2 is the final output of the neural network and will also be used interchangeably with Y hat so I know there was a lot of details but the key intuition to take away is that whereas for logistic regression we had this Z followed by a calculation and this new network here we just do it multiple times as a CV followed by a calculation and a Z followed by a calculation and then you finally compute the loss at the end and you remember that for the just regression we had in some backward calculation in order to compute derivatives are so confusing da easy and so on so in the same way in a new network we'll end up doing a backward calculation that looks like this and we jump you end up computing da 2 DZ 2 that allows you to compute so DW 2 DB 2 and so on in this order the right to less backward calculation that is denoting with the red arrows so thank you quick overview of what a new network websites based you take a logistic regression and repeating it twice I know there was a lot of new notation lot of new details don't worry about to get and follow everything we'll go into the details most slowly in the next few videos so let's go on to the next video we'll stop to talk about the neural network representation