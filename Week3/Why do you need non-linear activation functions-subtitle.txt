why does your new network need a nonlinear activation function turns out that for your new network to compute interesting functions you do need to take a nonlinear activation function less you want so just the for prop equations for the neural network why don't we just get rid of this get rid of the function G and set a1 equals Z 1 or alternatively you could say that G of Z is equal to Z right sometimes this is called the linear activation function maybe a better name for it would be the identity activation function because they're just outputs whatever was input for the purpose of this what if a2 was just equal to z2 it turns out if you do this then this model is just computing Y or Y hat as a linear function of your input features x2 take the first two equations if you have that a1 is equal to z1 is equal to w1 X plus B and if then a2 is equal to z2 is equal to W 2 a1 plus B then if you take the definition of a1 and plug it in there you find that a2 is equal to W 2 times W 1 X plus b1 a bit all right so this is um a 1 plus B 2 and so this simplifies to W 2 W 1 X plus W 2 b1 plus b2 so this it's just let's call this w prime b prime so it is just equal to w prime X plus B Prime if you were to use linear activation functions or we go to call them identity activation functions then the new network is just outputting a linear function of the input and we'll talk about deep networks later new networks with many many layers many many hidden layers and it turns out that if you use a linear activation function or alternatively if you don't have an activation function then no matter how many layers your neural network has always doing is just computing a linear activation function so you might as well not have any hidden layers some of the cases that briefly mentioned it turns out that if you have a linear activation function here and a sigmoid function here then this model is no more expressive than standard logistic regression without any hidden layer so I won't bother to prove that but you could try to do so if you want but the take-home is that a linear hidden layer is more or less useless because on the composition of two linear functions is itself a linear function so unless you throw a non-linearity in there then you're not computing more interesting functions even as you go deeper in the network there is just one place where you might use a linear activation function G of Z equals Z and that's if you are doing machine learning on a regression problem so if Y is a real number so for example if you're trying to predict housing prices so Y is a it's not 0 1 but it's a real number you know anywhere from zero dollars is a price of holes up to however expensive right house of kin I guess maybe however can be you know potentially millions of dollars so however however much houses cost in your data set but if Y takes on these real values then it might be OK to have a linear activation function here so that your output Y hat is also a real number going anywhere from minus infinity to plus infinity but then the hidden units should not use the new activation functions they could use relu or 10h or these you relu or maybe something else so the one place you might use a linear activation function others usually in the output layer but other than that using a linear activation function in a fitting layer except for some very special circumstances relating to compression that won't want to talk about using a linear activation function is extremely rare oh and of course today actually predicting housing prices as you saw on the week 1 video because housing prices are all non-negative perhaps even then you can use a value activation function so that your outputs Y hat are all greater than or equal to 0 so I hope that gives you a sense of why having a nonlinear activation function is a critical part of neural networks next we're going to start to talk about gradient descent and to do that to set up for discussion for gradient descent in the next video I want to show you how to estimate how to compute the slope of the derivative of individual activation functions so let's go on to the next video